import io
import xarray as xr
import pandas as pd
import snowflake.connector
from dagster import job, op, DynamicOut, DynamicOutput
import earthaccess
from datetime import datetime, timedelta

# ==========================
# Snowflake Config
# ==========================
SNOWFLAKE_ACCOUNT = "KBZQPZO-WX06551"
SNOWFLAKE_USER = "A7MEDESSO"
SNOWFLAKE_PASSWORD = "Ahmedesso@2005"
SNOWFLAKE_AUTHENTICATOR = "snowflake"
SNOWFLAKE_ROLE = "ACCOUNTADMIN"
SNOWFLAKE_WAREHOUSE = "NASA_WH"
SNOWFLAKE_DATABASE = "NASA_DB"
SNOWFLAKE_SCHEMA = "PUBLIC"

# Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
VARIABLES = ["PRECTOT", "CLDTT", "SLP"]  # Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø£Ù…Ø·Ø§Ø±ØŒ Ø§Ù„ØºØ·Ø§Ø¡ Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠØŒ Ø¶ØºØ· Ø³Ø·Ø­ Ø§Ù„Ø¨Ø­Ø±

# ==========================
# Helper Functions
# ==========================
def get_snowflake_connection():
    """Create Snowflake connection"""
    return snowflake.connector.connect(
        account=SNOWFLAKE_ACCOUNT,
        user=SNOWFLAKE_USER,
        password=SNOWFLAKE_PASSWORD,
        authenticator=SNOWFLAKE_AUTHENTICATOR,
        warehouse=SNOWFLAKE_WAREHOUSE,
        database=SNOWFLAKE_DATABASE,
        schema=SNOWFLAKE_SCHEMA,
        role=SNOWFLAKE_ROLE
    )

def get_season(month):
    """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ÙØµÙ„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø´Ù‡Ø±"""
    if month in [12, 1, 2]:
        return "Winter"
    elif month in [3, 4, 5]:
        return "Spring"
    elif month in [6, 7, 8]:
        return "Summer"
    else:
        return "Autumn"

def get_rain_intensity(precipitation_mm_day):
    """ØªØµÙ†ÙŠÙ Ø´Ø¯Ø© Ø§Ù„Ø£Ù…Ø·Ø§Ø±"""
    if precipitation_mm_day == 0:
        return "No Rain"
    elif precipitation_mm_day < 2.5:
        return "Light Rain"
    elif precipitation_mm_day < 7.5:
        return "Moderate Rain"
    elif precipitation_mm_day < 15:
        return "Heavy Rain"
    else:
        return "Very Heavy Rain"

def get_cloud_coverage(cloud_fraction):
    """ØªØµÙ†ÙŠÙ Ø§Ù„ØºØ·Ø§Ø¡ Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠ"""
    if cloud_fraction < 0.1:
        return "Clear"
    elif cloud_fraction < 0.25:
        return "Few Clouds"
    elif cloud_fraction < 0.5:
        return "Scattered Clouds"
    elif cloud_fraction < 0.75:
        return "Broken Clouds"
    else:
        return "Overcast"

def get_pressure_category(pressure_hpa):
    """ØªØµÙ†ÙŠÙ Ø¶ØºØ· Ø³Ø·Ø­ Ø§Ù„Ø¨Ø­Ø±"""
    if pressure_hpa < 980:
        return "Very Low"
    elif pressure_hpa < 1000:
        return "Low"
    elif pressure_hpa < 1020:
        return "Normal"
    elif pressure_hpa < 1040:
        return "High"
    else:
        return "Very High"

# ==========================
# DAGSTER OPS - STORMS & ATMOSPHERE DATA
# ==========================

@op(out=DynamicOut())
def search_nasa_files_storms_2022(context):
    """
    Ø¨Ø­Ø« Ø¹Ù† Ù…Ù„ÙØ§Øª NASA Ù„Ø³Ù†Ø© 2022 ÙƒØ§Ù…Ù„Ø© Ù„ÙƒÙ„ Ù…ØµØ±
    """
    context.log.info("ğŸ” Logging into NASA Earthdata...")
    auth = earthaccess.login(strategy="environment")
    
    context.log.info("ğŸ” Searching for NASA storm/atmosphere files for 2022...")
    
    # Ù…ØµØ± ÙƒÙ„Ù‡Ø§ - Ø­Ø¯ÙˆØ¯ Ø¬ØºØ±Ø§ÙÙŠØ© Ø´Ø§Ù…Ù„Ø©
    results = earthaccess.search_data(
        short_name="M2T1NXSLV",  # Ø£Ùˆ Ø§Ø³ØªØ®Ø¯Ù… dataset Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„Ø¹ÙˆØ§ØµÙ
        version="5.12.4",
        temporal=("2022-01-01", "2022-02-01"),
        bounding_box=(25.0, 22.0, 37.0, 32.0)  # Ù…ØµØ± ÙƒÙ„Ù‡Ø§
    )
    
    context.log.info(f"âœ… Found {len(results)} storm/atmosphere files for 2022")
    
    for idx, granule in enumerate(results):
        yield DynamicOutput(
            value=granule,
            mapping_key=f"file_{idx}"
        )

@op
def process_single_file_storms(context, granule) -> pd.DataFrame:
    """
    Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ù ÙˆØ§Ø­Ø¯ ÙˆØ­Ø³Ø§Ø¨ Ø§Ù„Ù…ØªÙˆØ³Ø·Ø§Øª Ø§Ù„ÙŠÙˆÙ…ÙŠØ© Ù„Ù„Ø¹ÙˆØ§ØµÙ ÙˆØ§Ù„ØºÙ„Ø§Ù Ø§Ù„Ø¬ÙˆÙŠ
    """
    try:
        context.log.info(f"ğŸ“¥ Streaming file: {granule['meta']['native-id']}")
        
        # ÙØªØ­ Ø§Ù„Ù…Ù„Ù Ù…Ø¨Ø§Ø´Ø±Ø© Ù…Ù† Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª
        file_stream = earthaccess.open([granule])[0]
        
        # ÙØªØ­ Ø§Ù„Ù€ dataset
        ds = xr.open_dataset(file_stream, engine="h5netcdf")
        
        all_daily_data = []
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒÙ„ Ù…ØªØºÙŠØ±
        for var in VARIABLES:
            if var in ds.variables:
                context.log.info(f"ğŸ“Š Processing variable: {var}")
                
                # Ù†Ø­ÙˆÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù€ DataFrame Ù…Ø¹ Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø§Ù„ÙˆÙ‚Øª
                df = ds[[var]].to_dataframe().reset_index()
                
                # Ù†ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø¹Ù…ÙˆØ¯ Ø§Ù„ÙˆÙ‚Øª
                if "time" not in df.columns:
                    context.log.warning("âš ï¸ No time column found")
                    continue
                
                # Ù†Ø­ÙˆÙ„ Ø§Ù„ÙˆÙ‚Øª Ù„Ù€ datetime
                df["time"] = pd.to_datetime(df["time"])
                
                # Ù†ØªØ£ÙƒØ¯ Ø£Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¶Ù…Ù† Ù†Ø·Ø§Ù‚ Ù…ØµØ±
                df = df[
                    (df["lat"] >= 22.0) & (df["lat"] <= 32.0) &
                    (df["lon"] >= 25.0) & (df["lon"] <= 37.0)
                ]
                
                if df.empty:
                    context.log.warning("âš ï¸ No data within Egypt boundaries")
                    continue
                
                # Ù†Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø§Ù„Ø³Ø§Ø¹Ø© ÙˆÙ†Ø­Ø³Ø¨ Ø§Ù„Ù…ØªÙˆØ³Ø· Ø§Ù„ÙŠÙˆÙ…ÙŠ
                df["date"] = df["time"].dt.date
                
                # Ù†Ø­Ø³Ø¨ Ø§Ù„Ù…ØªÙˆØ³Ø· Ø§Ù„ÙŠÙˆÙ…ÙŠ Ù„ÙƒÙ„ Ù…ÙˆÙ‚Ø¹
                daily_avg = df.groupby(["date", "lat", "lon"])[var].mean().reset_index()
                daily_avg["variable"] = var
                
                all_daily_data.append(daily_avg)
        
        ds.close()
        
        if not all_daily_data:
            context.log.warning(f"âš ï¸ No storm/atmosphere variables found in file")
            return pd.DataFrame()
        
        # Ù†Ø¬Ù…Ø¹ ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙŠÙˆÙ…ÙŠØ©
        combined = pd.concat(all_daily_data, ignore_index=True)
        context.log.info(f"âœ… Processed {len(combined)} daily storm/atmosphere records from file")
        
        return combined
        
    except Exception as e:
        context.log.error(f"âŒ Error processing file: {e}")
        return pd.DataFrame()

@op
def transform_daily_storms(context, df: pd.DataFrame) -> pd.DataFrame:
    """
    ØªØ­ÙˆÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¹ÙˆØ§ØµÙ ÙˆØ§Ù„ØºÙ„Ø§Ù Ø§Ù„Ø¬ÙˆÙŠ Ù„Ù„ØªØ®Ø²ÙŠÙ† ÙÙŠ Snowflake
    """
    if df.empty:
        return df
    
    context.log.info(f"ğŸ”„ Transforming {len(df)} daily storm/atmosphere records...")
    
    # Ù†ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
    required_cols = ["date", "variable", "lat", "lon"]
    missing_cols = [col for col in required_cols if col not in df.columns]
    
    if missing_cols:
        context.log.warning(f"âš ï¸ Missing columns: {missing_cols}")
        return pd.DataFrame()
    
    # Ù†Ø­Ø³Ø¨ Ø§Ù„Ù…ØªÙˆØ³Ø· Ø§Ù„ÙŠÙˆÙ…ÙŠ Ø§Ù„Ø´Ø§Ù…Ù„ Ù„ÙƒÙ„ Ù…ØªØºÙŠØ±
    daily_summary = (
        df.groupby(["date", "variable"])
        .agg({
            list(df.columns)[3]: 'mean',  # Ø£ÙˆÙ„ Ø¹Ù…ÙˆØ¯ Ø¨ÙŠØ§Ù†Ø§Øª
            'lat': 'count'
        })
        .reset_index()
    )
    
    # Ù†Ø¹ÙŠØ¯ ØªØ³Ù…ÙŠØ© Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ØªØºÙŠØ±
    if 'PRECTOT' in df['variable'].values:
        prec_data = daily_summary[daily_summary['variable'] == 'PRECTOT'].copy()
        prec_data.rename(columns={list(df.columns)[3]: 'avg_precipitation', 'lat': 'measurement_count'}, inplace=True)
        prec_data["precipitation_mm_day"] = prec_data["avg_precipitation"] * 86400  # ØªØ­ÙˆÙŠÙ„ Ù…Ù† kg/mÂ²/s Ø¥Ù„Ù‰ mm/day
        prec_data["rain_intensity"] = prec_data["precipitation_mm_day"].apply(get_rain_intensity)
    
    if 'CLDTT' in df['variable'].values:
        cloud_data = daily_summary[daily_summary['variable'] == 'CLDTT'].copy()
        cloud_data.rename(columns={list(df.columns)[3]: 'avg_cloud_fraction', 'lat': 'measurement_count'}, inplace=True)
        cloud_data["cloud_coverage"] = cloud_data["avg_cloud_fraction"].apply(get_cloud_coverage)
        cloud_data["cloud_percentage"] = cloud_data["avg_cloud_fraction"] * 100
    
    if 'SLP' in df['variable'].values:
        slp_data = daily_summary[daily_summary['variable'] == 'SLP'].copy()
        slp_data.rename(columns={list(df.columns)[3]: 'avg_sea_level_pressure', 'lat': 'measurement_count'}, inplace=True)
        slp_data["pressure_hpa"] = slp_data["avg_sea_level_pressure"] / 100.0
        slp_data["pressure_category"] = slp_data["pressure_hpa"].apply(get_pressure_category)
    
    # Ù†Ø¬Ù…Ø¹ ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    all_transformed = []
    if 'PRECTOT' in df['variable'].values:
        all_transformed.append(prec_data)
    if 'CLDTT' in df['variable'].values:
        all_transformed.append(cloud_data)
    if 'SLP' in df['variable'].values:
        all_transformed.append(slp_data)
    
    if not all_transformed:
        return pd.DataFrame()
    
    combined_result = pd.concat(all_transformed, ignore_index=True)
    
    # Ù†Ø¶ÙŠÙ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
    combined_result["year"] = pd.to_datetime(combined_result["date"]).dt.year
    combined_result["month"] = pd.to_datetime(combined_result["date"]).dt.month
    combined_result["day"] = pd.to_datetime(combined_result["date"]).dt.day
    combined_result["day_of_year"] = pd.to_datetime(combined_result["date"]).dt.dayofyear
    combined_result["day_name"] = pd.to_datetime(combined_result["date"]).dt.day_name()
    combined_result["season"] = combined_result["month"].apply(get_season)
    
    # Ù†Ø±ØªØ¨ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù†ÙˆØ¹ Ø§Ù„Ù…ØªØºÙŠØ±
    result_columns = ["date", "year", "month", "day", "day_of_year", "day_name", "season", "variable", "measurement_count"]
    
    # Ù†Ø¶ÙŠÙ Ø£Ø¹Ù…Ø¯Ø© Ø¥Ø¶Ø§ÙÙŠØ© Ø­Ø³Ø¨ Ø§Ù„Ù…ØªØºÙŠØ±
    if 'PRECTOT' in df['variable'].values:
        result_columns.extend(["avg_precipitation", "precipitation_mm_day", "rain_intensity"])
    if 'CLDTT' in df['variable'].values:
        result_columns.extend(["avg_cloud_fraction", "cloud_percentage", "cloud_coverage"])
    if 'SLP' in df['variable'].values:
        result_columns.extend(["avg_sea_level_pressure", "pressure_hpa", "pressure_category"])
    
    # Ù†Ø£Ø®Ø° Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…ØªØ§Ø­Ø© ÙÙ‚Ø·
    available_columns = [col for col in result_columns if col in combined_result.columns]
    result = combined_result[available_columns]
    
    context.log.info(f"âœ… Transformed to {len(result)} daily storm/atmosphere summary records")
    
    return result

@op
def load_daily_storms_to_snowflake(context, df: pd.DataFrame):
    """
    ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¹ÙˆØ§ØµÙ ÙˆØ§Ù„ØºÙ„Ø§Ù Ø§Ù„Ø¬ÙˆÙŠ Ù„Ù€ Snowflake
    """
    if df.empty:
        context.log.warning("âš ï¸ Empty dataframe - skipping load")
        return "skipped"
    
    context.log.info(f"ğŸ“¤ Loading {len(df)} daily storm/atmosphere records to Snowflake...")
    
    try:
        conn = get_snowflake_connection()
        cur = conn.cursor()
        
        # Ù†Ù†Ø´Ø¦ Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ù„Ùˆ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯
        cur.execute("""
            CREATE TABLE IF NOT EXISTS NASA_DAILY_STORMS_ATMOSPHERE (
                date DATE,
                year INT,
                month INT,
                day INT,
                day_of_year INT,
                day_name STRING,
                season STRING,
                variable STRING,
                avg_precipitation FLOAT,
                precipitation_mm_day FLOAT,
                rain_intensity STRING,
                avg_cloud_fraction FLOAT,
                cloud_percentage FLOAT,
                cloud_coverage STRING,
                avg_sea_level_pressure FLOAT,
                pressure_hpa FLOAT,
                pressure_category STRING,
                measurement_count INT,
                loaded_at TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()
            )
        """)
        
        # batch insert Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙŠÙˆÙ…ÙŠØ©
        insert_query = """
            INSERT INTO NASA_DAILY_STORMS_ATMOSPHERE 
            (date, year, month, day, day_of_year, day_name, season, variable, 
             avg_precipitation, precipitation_mm_day, rain_intensity,
             avg_cloud_fraction, cloud_percentage, cloud_coverage,
             avg_sea_level_pressure, pressure_hpa, pressure_category, measurement_count) 
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
        """
        
        # Ù†Ø­Ø¶Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø¥Ø¯Ø®Ø§Ù„ (Ù…Ø¹ ØªØ¹ÙˆÙŠØ¶ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© Ø¨Ù€ NULL)
        data_to_insert = []
        for _, row in df.iterrows():
            data_to_insert.append((
                row["date"], 
                int(row["year"]), 
                int(row["month"]), 
                int(row["day"]), 
                int(row["day_of_year"]),
                row["day_name"],
                row["season"],
                row["variable"], 
                float(row["avg_precipitation"]) if "avg_precipitation" in row and pd.notna(row["avg_precipitation"]) else None,
                float(row["precipitation_mm_day"]) if "precipitation_mm_day" in row and pd.notna(row["precipitation_mm_day"]) else None,
                row["rain_intensity"] if "rain_intensity" in row and pd.notna(row["rain_intensity"]) else None,
                float(row["avg_cloud_fraction"]) if "avg_cloud_fraction" in row and pd.notna(row["avg_cloud_fraction"]) else None,
                float(row["cloud_percentage"]) if "cloud_percentage" in row and pd.notna(row["cloud_percentage"]) else None,
                row["cloud_coverage"] if "cloud_coverage" in row and pd.notna(row["cloud_coverage"]) else None,
                float(row["avg_sea_level_pressure"]) if "avg_sea_level_pressure" in row and pd.notna(row["avg_sea_level_pressure"]) else None,
                float(row["pressure_hpa"]) if "pressure_hpa" in row and pd.notna(row["pressure_hpa"]) else None,
                row["pressure_category"] if "pressure_category" in row and pd.notna(row["pressure_category"]) else None,
                int(row["measurement_count"])
            ))
        
        # Ø¥Ø¯Ø®Ø§Ù„ Ø¬Ù…Ø§Ø¹ÙŠ
        cur.executemany(insert_query, data_to_insert)
        conn.commit()
        
        context.log.info(f"âœ… Successfully loaded {len(df)} daily storm/atmosphere records")
        
        cur.close()
        conn.close()
        
        return "success"
        
    except Exception as e:
        context.log.error(f"âŒ Error loading to Snowflake: {e}")
        raise

# ==========================
# DAGSTER JOB - STORMS & ATMOSPHERE PIPELINE 2022
# ==========================

@job
def nasa_daily_storms_atmosphere_2022_pipeline():
    """
    Pipeline Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¹ÙˆØ§ØµÙ ÙˆØ§Ù„ØºÙ„Ø§Ù Ø§Ù„Ø¬ÙˆÙŠ Ø§Ù„ÙŠÙˆÙ…ÙŠØ© Ù„Ø³Ù†Ø© 2022 Ù„ÙƒÙ„ Ù…ØµØ±
    """
    # Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ù„ÙØ§Øª Ù„Ø³Ù†Ø© 2022
    files = search_nasa_files_storms_2022()
    
    # Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒÙ„ Ù…Ù„Ù ÙˆØ­Ø³Ø§Ø¨ Ø§Ù„Ù…ØªÙˆØ³Ø·Ø§Øª Ø§Ù„ÙŠÙˆÙ…ÙŠØ©
    processed = files.map(process_single_file_storms)
    
    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙŠÙˆÙ…ÙŠØ©
    transformed = processed.map(transform_daily_storms)
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙŠÙˆÙ…ÙŠØ© Ù„Ù€ Snowflake
    transformed.map(load_daily_storms_to_snowflake)
