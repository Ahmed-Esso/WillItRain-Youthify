import io
import xarray as xr
import pandas as pd
import snowflake.connector
from dagster import job, op, DynamicOut, DynamicOutput
import earthaccess
from datetime import datetime, timedelta
import math
import numpy as np

# ==========================
# Snowflake Config
# ==========================
SNOWFLAKE_ACCOUNT = "KBZQPZO-WX06551"
SNOWFLAKE_USER = "A7MEDESSO"
SNOWFLAKE_PASSWORD = "Ahmedesso@2005"
SNOWFLAKE_AUTHENTICATOR = "snowflake"
SNOWFLAKE_ROLE = "ACCOUNTADMIN"
SNOWFLAKE_WAREHOUSE = "NASA_WH"
SNOWFLAKE_DATABASE = "NASA_DB"
SNOWFLAKE_SCHEMA = "PUBLIC"

VARIABLES = ["U10M", "V10M", "PS", "SLP"]  # Ø³Ø±Ø¹Ø© Ø§Ù„Ø±ÙŠØ§Ø­ (Ù…ÙƒÙˆÙ†Ø§Øª x, y)ØŒ Ø§Ù„Ø¶ØºØ· Ø§Ù„Ø³Ø·Ø­ÙŠØŒ Ø¶ØºØ· Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø¨Ø­Ø±

# ==========================
# Helper Function
# ==========================
def get_snowflake_connection():
    """Create Snowflake connection"""
    return snowflake.connector.connect(
        account=SNOWFLAKE_ACCOUNT,
        user=SNOWFLAKE_USER,
        password=SNOWFLAKE_PASSWORD,
        authenticator=SNOWFLAKE_AUTHENTICATOR,
        warehouse=SNOWFLAKE_WAREHOUSE,
        database=SNOWFLAKE_DATABASE,
        schema=SNOWFLAKE_SCHEMA,
        role=SNOWFLAKE_ROLE
    )

def calculate_wind_speed(u_component, v_component):
    """Ø­Ø³Ø§Ø¨ Ø³Ø±Ø¹Ø© Ø§Ù„Ø±ÙŠØ§Ø­ Ø§Ù„ÙƒÙ„ÙŠØ© Ù…Ù† Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª U Ùˆ V"""
    return math.sqrt(u_component**2 + v_component**2)

def calculate_wind_direction(u_component, v_component):
    """Ø­Ø³Ø§Ø¨ Ø§ØªØ¬Ø§Ù‡ Ø§Ù„Ø±ÙŠØ§Ø­ Ù…Ù† Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª U Ùˆ V (Ø¨Ø§Ù„Ø¯Ø±Ø¬Ø§Øª)"""
    direction = math.degrees(math.atan2(u_component, v_component))
    if direction < 0:
        direction += 360
    return direction

def calculate_circular_mean(angles):
    """Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ØªÙˆØ³Ø· Ø§Ù„Ø¯Ø§Ø¦Ø±ÙŠ Ù„Ù„Ø²ÙˆØ§ÙŠØ§ (Ù„Ø§ØªØ¬Ø§Ù‡ Ø§Ù„Ø±ÙŠØ§Ø­)"""
    sin_sum = sum(math.sin(math.radians(angle)) for angle in angles)
    cos_sum = sum(math.cos(math.radians(angle)) for angle in angles)
    
    if sin_sum == 0 and cos_sum == 0:
        return 0
    
    mean_angle = math.degrees(math.atan2(sin_sum, cos_sum))
    if mean_angle < 0:
        mean_angle += 360
    
    return mean_angle

# ==========================
# DAGSTER OPS - DAILY AVERAGE
# ==========================

@op(out=DynamicOut())
def search_nasa_wind_files(context):
    """
    Ø¨Ø­Ø« Ø¹Ù† Ù…Ù„ÙØ§Øª NASA Ù„Ù„ÙØªØ±Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„Ù„Ø±ÙŠØ§Ø­ ÙˆØ§Ù„Ø¶ØºØ·
    """
    context.log.info("ğŸ” Logging into NASA Earthdata...")
    auth = earthaccess.login(strategy="environment")
    
    context.log.info("ğŸ” Searching for NASA wind files...")
    results = earthaccess.search_data(
        short_name="M2T1NXSLV",
        version="5.12.4",
        temporal=("2022-01-01", "2023-01-01"),
        bounding_box=(24.70, 22.00, 37.35, 31.67)  # Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ù‚Ø§Ù‡Ø±Ø©
    )
    
    context.log.info(f"âœ… Found {len(results)} wind files")
    
    for idx, granule in enumerate(results):
        yield DynamicOutput(
            value=granule,
            mapping_key=f"wind_file_{idx}"
        )


@op
def process_single_wind_file(context, granule) -> pd.DataFrame:
    """
    Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ù ÙˆØ§Ø­Ø¯ ÙˆØ­Ø³Ø§Ø¨ Ø§Ù„Ù…ØªÙˆØ³Ø· Ø§Ù„ÙŠÙˆÙ…ÙŠ Ù„Ù„Ø±ÙŠØ§Ø­ ÙˆØ§Ù„Ø¶ØºØ·
    """
    try:
        context.log.info(f"ğŸ“¥ Streaming wind file: {granule['meta']['native-id']}")
        
        file_stream = earthaccess.open([granule])[0]
        ds = xr.open_dataset(file_stream, engine="h5netcdf")
        
        all_daily_data = []
        
        for var in VARIABLES:
            if var in ds.variables:
                context.log.info(f"ğŸ“Š Processing wind variable: {var}")
                
                df = ds[[var]].to_dataframe().reset_index()
                
                if "time" not in df.columns:
                    context.log.warning("âš ï¸ No time column found")
                    continue
                
                df["time"] = pd.to_datetime(df["time"])
                df["date"] = df["time"].dt.date
                
                daily_avg = df.groupby(["date", "lat", "lon"])[var].mean().reset_index()
                daily_avg["variable"] = var
                
                all_daily_data.append(daily_avg)
            else:
                context.log.warning(f"âš ï¸ Wind variable {var} not found in dataset")
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø®Ø§ØµØ© Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø±ÙŠØ§Ø­ Ù„Ø­Ø³Ø§Ø¨ Ø³Ø±Ø¹Ø© ÙˆØ§ØªØ¬Ø§Ù‡ Ø§Ù„Ø±ÙŠØ§Ø­
        if "U10M" in ds.variables and "V10M" in ds.variables:
            context.log.info("ğŸŒ¬ï¸ Processing wind speed and direction...")
            
            # Ù†Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª U Ùˆ V Ù…Ø¹Ø§Ù‹
            wind_df = ds[["U10M", "V10M"]].to_dataframe().reset_index()
            
            if "time" in wind_df.columns:
                wind_df["time"] = pd.to_datetime(wind_df["time"])
                wind_df["date"] = wind_df["time"].dt.date
                
                # Ù†Ø­Ø³Ø¨ Ø³Ø±Ø¹Ø© Ø§Ù„Ø±ÙŠØ§Ø­ ÙˆØ§ØªØ¬Ø§Ù‡Ù‡Ø§ Ù„ÙƒÙ„ Ù†Ù‚Ø·Ø©
                wind_df["WIND_SPEED"] = wind_df.apply(
                    lambda row: calculate_wind_speed(row["U10M"], row["V10M"]), 
                    axis=1
                )
                
                wind_df["WIND_DIRECTION"] = wind_df.apply(
                    lambda row: calculate_wind_direction(row["U10M"], row["V10M"]), 
                    axis=1
                )
                
                # Ø§Ù„Ù…ØªÙˆØ³Ø· Ø§Ù„ÙŠÙˆÙ…ÙŠ Ù„Ø³Ø±Ø¹Ø© Ø§Ù„Ø±ÙŠØ§Ø­
                wind_speed_daily = wind_df.groupby(["date", "lat", "lon"])["WIND_SPEED"].mean().reset_index()
                wind_speed_daily["variable"] = "WIND_SPEED"
                all_daily_data.append(wind_speed_daily)
                
                # Ø§Ù„Ù…ØªÙˆØ³Ø· Ø§Ù„ÙŠÙˆÙ…ÙŠ Ù„Ø§ØªØ¬Ø§Ù‡ Ø§Ù„Ø±ÙŠØ§Ø­ (Ù†Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…ØªÙˆØ³Ø· Ø§Ù„Ø¯Ø§Ø¦Ø±ÙŠ)
                wind_direction_daily = wind_df.groupby(["date", "lat", "lon"]).apply(
                    lambda x: pd.Series({
                        'WIND_DIRECTION': calculate_circular_mean(x['WIND_DIRECTION'])
                    })
                ).reset_index()
                wind_direction_daily["variable"] = "WIND_DIRECTION"
                all_daily_data.append(wind_direction_daily)
        
        ds.close()
        
        if not all_daily_data:
            context.log.warning(f"âš ï¸ No wind variables found in file")
            return pd.DataFrame()
        
        combined = pd.concat(all_daily_data, ignore_index=True)
        context.log.info(f"âœ… Processed {len(combined)} daily wind records from file")
        
        return combined
        
    except Exception as e:
        context.log.error(f"âŒ Error processing wind file: {e}")
        return pd.DataFrame()


@op
def clean_wind_dataframe(context, df: pd.DataFrame) -> pd.DataFrame:
    """
    ØªÙ†Ø¸ÙŠÙ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø±ÙŠØ§Ø­ ÙˆØ§Ù„Ø¶ØºØ· ÙˆØ¥Ø²Ø§Ù„Ø© Ø§Ù„Ù‚ÙŠÙ… NaN
    """
    if df.empty:
        return df
    
    context.log.info(f"ğŸ§¹ Cleaning {len(df)} wind records...")
    
    # Ù†Ø³Ø®Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªÙ†Ø¸ÙŠÙ
    cleaned_df = df.copy()
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØµÙÙˆÙ Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ NaN ÙÙŠ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
    initial_count = len(cleaned_df)
    cleaned_df = cleaned_df.dropna(subset=['date', 'variable', 'lat', 'lon'])
    
    # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ NaN ÙÙŠ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø±Ù‚Ù…ÙŠØ© Ø¨ØµÙØ±
    numeric_columns = cleaned_df.select_dtypes(include=['number']).columns
    for col in numeric_columns:
        nan_count = cleaned_df[col].isna().sum()
        if nan_count > 0:
            context.log.info(f"ğŸ”§ Replacing {nan_count} NaN values in {col} with 0")
            cleaned_df[col] = cleaned_df[col].fillna(0)
    
    removed_count = initial_count - len(cleaned_df)
    if removed_count > 0:
        context.log.info(f"ğŸ—‘ï¸ Removed {removed_count} wind records with missing essential data")
    
    context.log.info(f"âœ… Cleaned {len(cleaned_df)} wind records")
    
    return cleaned_df


@op
def transform_wind_data(context, df: pd.DataFrame) -> pd.DataFrame:
    """
    ØªØ­ÙˆÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø±ÙŠØ§Ø­ ÙˆØ§Ù„Ø¶ØºØ· Ø§Ù„ÙŠÙˆÙ…ÙŠØ© Ù„Ù„ØªØ®Ø²ÙŠÙ† ÙÙŠ Snowflake
    """
    if df.empty:
        return df
    
    context.log.info(f"ğŸ”„ Transforming {len(df)} daily wind records...")
    
    required_cols = ["date", "variable", "lat", "lon"]
    missing_cols = [col for col in required_cols if col not in df.columns]
    
    if missing_cols:
        context.log.warning(f"âš ï¸ Missing columns: {missing_cols}")
        return pd.DataFrame()
    
    # Ù†Ø­Ø¯Ø¯ Ø§Ø³Ù… Ø¹Ù…ÙˆØ¯ Ø§Ù„Ù‚ÙŠÙ…Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ØªØºÙŠØ±
    value_column = [col for col in df.columns if col not in ['date', 'variable', 'lat', 'lon', 'time']][0]
    
    daily_summary = (
        df.groupby(["date", "variable"])
        .agg({
            value_column: 'mean',
            'lat': 'count'
        })
        .reset_index()
    )
    
    daily_summary.rename(columns={
        value_column: 'avg_value',
        'lat': 'measurement_count'
    }, inplace=True)
    
    daily_summary["year"] = pd.to_datetime(daily_summary["date"]).dt.year
    daily_summary["month"] = pd.to_datetime(daily_summary["date"]).dt.month
    daily_summary["day"] = pd.to_datetime(daily_summary["date"]).dt.day
    
    result = daily_summary[[
        "date", "year", "month", "day", "variable", 
        "avg_value", "measurement_count"
    ]]
    
    context.log.info(f"âœ… Transformed to {len(result)} daily wind summary records")
    
    return result


@op
def validate_wind_data_before_load(context, df: pd.DataFrame) -> pd.DataFrame:
    """
    ÙØ­Øµ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø±ÙŠØ§Ø­ ÙˆØ§Ù„Ø¶ØºØ· Ù‚Ø¨Ù„ Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ù„Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ù…Ø¨ÙƒØ±Ø§Ù‹
    """
    if df.empty:
        context.log.warning("âš ï¸ Empty wind dataframe in validation")
        return df
    
    context.log.info(f"ğŸ” Validating {len(df)} wind records before load...")
    
    # ÙØ­Øµ Ø§Ù„Ù‚ÙŠÙ… NaN
    nan_check = df.isna().sum()
    total_nan = nan_check.sum()
    
    if total_nan > 0:
        context.log.warning(f"âš ï¸ Found {total_nan} NaN values in wind dataframe:")
        for column, count in nan_check.items():
            if count > 0:
                context.log.warning(f"   - {column}: {count} NaN values")
    
    # ÙØ­Øµ Ø§Ù„Ù‚ÙŠÙ… ÙÙŠ avg_value
    if "avg_value" in df.columns:
        avg_value_stats = df["avg_value"].describe()
        context.log.info(f"ğŸ“Š avg_value statistics: count={avg_value_stats['count']}, mean={avg_value_stats['mean']:.2f}")
        
        # Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ù‚ÙŠÙ… ØºÙŠØ± Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©
        infinite_values = np.isinf(df["avg_value"]).sum()
        if infinite_values > 0:
            context.log.warning(f"âš ï¸ Found {infinite_values} infinite values in avg_value")
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØµÙÙˆÙ Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ NaN ÙÙŠ Ø£Ø¹Ù…Ø¯Ø© Ø£Ø³Ø§Ø³ÙŠØ©
    initial_count = len(df)
    essential_columns = ['date', 'variable', 'avg_value']
    cleaned_df = df.dropna(subset=essential_columns)
    removed_count = initial_count - len(cleaned_df)
    
    if removed_count > 0:
        context.log.warning(f"ğŸ—‘ï¸ Removed {removed_count} wind records with missing essential data")
    
    context.log.info(f"âœ… Wind validation complete. {len(cleaned_df)} valid records remaining")
    
    return cleaned_df


@op
def load_wind_to_snowflake(context, df: pd.DataFrame):
    """
    ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø±ÙŠØ§Ø­ ÙˆØ§Ù„Ø¶ØºØ· Ø§Ù„ÙŠÙˆÙ…ÙŠØ© Ù„Ù€ Snowflake
    """
    if df.empty:
        context.log.warning("âš ï¸ Empty wind dataframe - skipping load")
        return {"status": "skipped", "loaded_count": 0, "file_info": "empty"}

    context.log.info(f"ğŸ“¤ Preparing to load {len(df)} daily wind records to Snowflake...")
    
    try:
        conn = get_snowflake_connection()
        cur = conn.cursor()
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
        cur.execute("""
            CREATE TABLE IF NOT EXISTS NASA_DAILY_WIND_PRESSURE (
                date DATE,
                year INT,
                month INT,
                day INT,
                variable STRING,
                avg_value FLOAT,
                measurement_count INT,
                loaded_at TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()
            )
        """)
        
        insert_query = """
            INSERT INTO NASA_DAILY_WIND_PRESSURE 
            (date, year, month, day, variable, avg_value, measurement_count) 
            VALUES (%s, %s, %s, %s, %s, %s, %s)
        """
        
        # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© NaN
        successful_inserts = 0
        failed_inserts = 0
        error_messages = []
        
        for index, row in df.iterrows():
            try:
                # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… NaN
                avg_value = row["avg_value"]
                if pd.isna(avg_value) or pd.isnull(avg_value):
                    context.log.warning(f"âš ï¸ Found NaN value in row {index}, skipping this row")
                    failed_inserts += 1
                    continue
                
                measurement_count = row["measurement_count"]
                if pd.isna(measurement_count) or pd.isnull(measurement_count):
                    measurement_count = 0
                
                # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
                if pd.isna(row["date"]) or pd.isna(row["variable"]):
                    context.log.warning(f"âš ï¸ Missing essential data in row {index}, skipping")
                    failed_inserts += 1
                    continue
                
                # Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„ØµÙ
                cur.execute(insert_query, (
                    row["date"], 
                    int(row["year"]), 
                    int(row["month"]), 
                    int(row["day"]), 
                    row["variable"], 
                    float(avg_value),
                    int(measurement_count)
                ))
                successful_inserts += 1
                
            except Exception as row_error:
                failed_inserts += 1
                error_msg = f"Error in row {index}: {str(row_error)}"
                error_messages.append(error_msg)
                context.log.warning(f"âš ï¸ {error_msg}")
                continue  # Ø§Ø³ØªÙ…Ø±Ø§Ø± Ù…Ø¹ Ø§Ù„ØµÙÙˆÙ Ø§Ù„ØªØ§Ù„ÙŠØ©
        
        conn.commit()
        
        # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        if successful_inserts > 0:
            context.log.info(f"âœ… Successfully loaded {successful_inserts} daily wind records")
        
        if failed_inserts > 0:
            context.log.warning(f"âš ï¸ Failed to load {failed_inserts} wind records due to errors")
        
        if error_messages:
            context.log.info("ğŸ“‹ Wind error summary:")
            for msg in error_messages[:5]:  # Ø§Ø·Ø¨Ø¹ Ø£ÙˆÙ„ 5 Ø£Ø®Ø·Ø§Ø¡ ÙÙ‚Ø·
                context.log.info(f"   - {msg}")
            if len(error_messages) > 5:
                context.log.info(f"   ... and {len(error_messages) - 5} more errors")
        
        cur.close()
        conn.close()
        
        return {
            "status": "partial_success" if failed_inserts > 0 else "success",
            "loaded_count": successful_inserts,
            "failed_count": failed_inserts,
            "total_records": len(df)
        }
        
    except Exception as e:
        context.log.error(f"âŒ Critical error loading wind data to Snowflake: {e}")
        return {
            "status": "failed",
            "loaded_count": 0,
            "failed_count": len(df),
            "error": str(e)
        }


@job
def nasa_daily_wind_pipeline():
    """
    Pipeline Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø±ÙŠØ§Ø­ ÙˆØ§Ù„Ø¶ØºØ· Ø§Ù„ÙŠÙˆÙ…ÙŠØ©
    """
    files = search_nasa_wind_files()
    processed = files.map(process_single_wind_file)
    cleaned = processed.map(clean_wind_dataframe)
    transformed = cleaned.map(transform_wind_data)
    validated = transformed.map(validate_wind_data_before_load)
    validated.map(load_wind_to_snowflake)


if __name__ == "__main__":
    from dagster import execute_pipeline
    result = execute_pipeline(nasa_daily_wind_pipeline)
    print(f"âœ… Wind pipeline finished: {result.success}")
