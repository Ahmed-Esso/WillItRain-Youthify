import io
import xarray as xr
import pandas as pd
import snowflake.connector
from dagster import job, op, DynamicOut, DynamicOutput
import earthaccess
from datetime import datetime, timedelta
import numpy as np

# ==========================
# Snowflake Config
# ==========================
SNOWFLAKE_ACCOUNT = "KBZQPZO-WX06551"
SNOWFLAKE_USER = "A7MEDESSO"
SNOWFLAKE_PASSWORD = "Ahmedesso@2005"
SNOWFLAKE_AUTHENTICATOR = "snowflake"
SNOWFLAKE_ROLE = "ACCOUNTADMIN"
SNOWFLAKE_WAREHOUSE = "NASA_WH"
SNOWFLAKE_DATABASE = "NASA_DB"
SNOWFLAKE_SCHEMA = "PUBLIC"

VARIABLES = ["QV2M", "T2MDEW", "T2MWET"]  # Ø§Ù„Ø±Ø·ÙˆØ¨Ø©ØŒ Ù†Ù‚Ø·Ø© Ø§Ù„Ù†Ø¯Ù‰ØŒ Ø§Ù„Ø­Ø±Ø§Ø±Ø© Ø§Ù„Ø±Ø·Ø¨Ø©

# ==========================
# Helper Function
# ==========================
def get_snowflake_connection():
    """Create Snowflake connection"""
    return snowflake.connector.connect(
        account=SNOWFLAKE_ACCOUNT,
        user=SNOWFLAKE_USER,
        password=SNOWFLAKE_PASSWORD,
        authenticator=SNOWFLAKE_AUTHENTICATOR,
        warehouse=SNOWFLAKE_WAREHOUSE,
        database=SNOWFLAKE_DATABASE,
        schema=SNOWFLAKE_SCHEMA,
        role=SNOWFLAKE_ROLE
    )

# ==========================
# DAGSTER OPS - DAILY AVERAGE
# ==========================

@op(out=DynamicOut())
def search_nasa_weather_files(context):
    """
    Ø¨Ø­Ø« Ø¹Ù† Ù…Ù„ÙØ§Øª NASA Ù„Ù„ÙØªØ±Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„Ù„Ø±Ø·ÙˆØ¨Ø© ÙˆØ¯Ø±Ø¬Ø© Ø§Ù„Ø­Ø±Ø§Ø±Ø©
    """
    context.log.info("ğŸ” Logging into NASA Earthdata...")
    auth = earthaccess.login(strategy="environment")
    
    context.log.info("ğŸ” Searching for NASA weather files...")
    results = earthaccess.search_data(
        short_name="M2T1NXSLV",
        version="5.12.4",
        temporal=("2022-01-01", "2023-01-01"),
        bounding_box=(24.70, 22.00, 37.35, 31.67)  # Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ù‚Ø§Ù‡Ø±Ø©
    )
    
    context.log.info(f"âœ… Found {len(results)} weather files")
    
    for idx, granule in enumerate(results):
        yield DynamicOutput(
            value=granule,
            mapping_key=f"weather_file_{idx}"
        )


@op
def process_single_weather_file(context, granule) -> pd.DataFrame:
    """
    Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ù ÙˆØ§Ø­Ø¯ ÙˆØ­Ø³Ø§Ø¨ Ø§Ù„Ù…ØªÙˆØ³Ø· Ø§Ù„ÙŠÙˆÙ…ÙŠ Ù„Ù„Ø±Ø·ÙˆØ¨Ø© ÙˆØ¯Ø±Ø¬Ø© Ø§Ù„Ø­Ø±Ø§Ø±Ø©
    """
    try:
        context.log.info(f"ğŸ“¥ Streaming weather file: {granule['meta']['native-id']}")
        
        file_stream = earthaccess.open([granule])[0]
        ds = xr.open_dataset(file_stream, engine="h5netcdf")
        
        all_daily_data = []
        
        for var in VARIABLES:
            if var in ds.variables:
                context.log.info(f"ğŸ“Š Processing weather variable: {var}")
                
                df = ds[[var]].to_dataframe().reset_index()
                
                if "time" not in df.columns:
                    context.log.warning("âš ï¸ No time column found")
                    continue
                
                df["time"] = pd.to_datetime(df["time"])
                df["date"] = df["time"].dt.date
                
                daily_avg = df.groupby(["date", "lat", "lon"])[var].mean().reset_index()
                daily_avg["variable"] = var
                
                all_daily_data.append(daily_avg)
            else:
                context.log.warning(f"âš ï¸ Weather variable {var} not found in dataset")
        
        ds.close()
        
        if not all_daily_data:
            context.log.warning(f"âš ï¸ No weather variables found in file")
            return pd.DataFrame()
        
        combined = pd.concat(all_daily_data, ignore_index=True)
        context.log.info(f"âœ… Processed {len(combined)} daily weather records from file")
        
        return combined
        
    except Exception as e:
        context.log.error(f"âŒ Error processing weather file: {e}")
        return pd.DataFrame()


@op
def clean_weather_dataframe(context, df: pd.DataFrame) -> pd.DataFrame:
    """
    ØªÙ†Ø¸ÙŠÙ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø±Ø·ÙˆØ¨Ø© ÙˆØ¯Ø±Ø¬Ø© Ø§Ù„Ø­Ø±Ø§Ø±Ø© ÙˆØ¥Ø²Ø§Ù„Ø© Ø§Ù„Ù‚ÙŠÙ… NaN
    """
    if df.empty:
        return df
    
    context.log.info(f"ğŸ§¹ Cleaning {len(df)} weather records...")
    
    # Ù†Ø³Ø®Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªÙ†Ø¸ÙŠÙ
    cleaned_df = df.copy()
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØµÙÙˆÙ Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ NaN ÙÙŠ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
    initial_count = len(cleaned_df)
    cleaned_df = cleaned_df.dropna(subset=['date', 'variable', 'lat', 'lon'])
    
    # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ NaN ÙÙŠ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø±Ù‚Ù…ÙŠØ© Ø¨ØµÙØ±
    numeric_columns = cleaned_df.select_dtypes(include=['number']).columns
    for col in numeric_columns:
        nan_count = cleaned_df[col].isna().sum()
        if nan_count > 0:
            context.log.info(f"ğŸ”§ Replacing {nan_count} NaN values in {col} with 0")
            cleaned_df[col] = cleaned_df[col].fillna(0)
    
    removed_count = initial_count - len(cleaned_df)
    if removed_count > 0:
        context.log.info(f"ğŸ—‘ï¸ Removed {removed_count} weather records with missing essential data")
    
    context.log.info(f"âœ… Cleaned {len(cleaned_df)} weather records")
    
    return cleaned_df


@op
def transform_weather_data(context, df: pd.DataFrame) -> pd.DataFrame:
    """
    ØªØ­ÙˆÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø±Ø·ÙˆØ¨Ø© ÙˆØ¯Ø±Ø¬Ø© Ø§Ù„Ø­Ø±Ø§Ø±Ø© Ø§Ù„ÙŠÙˆÙ…ÙŠØ© Ù„Ù„ØªØ®Ø²ÙŠÙ† ÙÙŠ Snowflake
    """
    if df.empty:
        return df
    
    context.log.info(f"ğŸ”„ Transforming {len(df)} daily weather records...")
    
    required_cols = ["date", "variable", "lat", "lon"]
    missing_cols = [col for col in required_cols if col not in df.columns]
    
    if missing_cols:
        context.log.warning(f"âš ï¸ Missing columns: {missing_cols}")
        return pd.DataFrame()
    
    # Ù†Ø­Ø¯Ø¯ Ø§Ø³Ù… Ø¹Ù…ÙˆØ¯ Ø§Ù„Ù‚ÙŠÙ…Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ØªØºÙŠØ±
    value_column = [col for col in df.columns if col not in ['date', 'variable', 'lat', 'lon', 'time']][0]
    
    daily_summary = (
        df.groupby(["date", "variable"])
        .agg({
            value_column: 'mean',
            'lat': 'count'
        })
        .reset_index()
    )
    
    daily_summary.rename(columns={
        value_column: 'avg_value',
        'lat': 'measurement_count'
    }, inplace=True)
    
    daily_summary["year"] = pd.to_datetime(daily_summary["date"]).dt.year
    daily_summary["month"] = pd.to_datetime(daily_summary["date"]).dt.month
    daily_summary["day"] = pd.to_datetime(daily_summary["date"]).dt.day
    
    result = daily_summary[[
        "date", "year", "month", "day", "variable", 
        "avg_value", "measurement_count"
    ]]
    
    context.log.info(f"âœ… Transformed to {len(result)} daily weather summary records")
    
    return result


@op
def validate_weather_data_before_load(context, df: pd.DataFrame) -> pd.DataFrame:
    """
    ÙØ­Øµ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø±Ø·ÙˆØ¨Ø© ÙˆØ¯Ø±Ø¬Ø© Ø§Ù„Ø­Ø±Ø§Ø±Ø© Ù‚Ø¨Ù„ Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ù„Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ù…Ø¨ÙƒØ±Ø§Ù‹
    """
    if df.empty:
        context.log.warning("âš ï¸ Empty weather dataframe in validation")
        return df
    
    context.log.info(f"ğŸ” Validating {len(df)} weather records before load...")
    
    # ÙØ­Øµ Ø§Ù„Ù‚ÙŠÙ… NaN
    nan_check = df.isna().sum()
    total_nan = nan_check.sum()
    
    if total_nan > 0:
        context.log.warning(f"âš ï¸ Found {total_nan} NaN values in weather dataframe:")
        for column, count in nan_check.items():
            if count > 0:
                context.log.warning(f"   - {column}: {count} NaN values")
    
    # ÙØ­Øµ Ø§Ù„Ù‚ÙŠÙ… ÙÙŠ avg_value
    if "avg_value" in df.columns:
        avg_value_stats = df["avg_value"].describe()
        context.log.info(f"ğŸ“Š avg_value statistics: count={avg_value_stats['count']}, mean={avg_value_stats['mean']:.2f}")
        
        # Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ù‚ÙŠÙ… ØºÙŠØ± Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©
        infinite_values = np.isinf(df["avg_value"]).sum()
        if infinite_values > 0:
            context.log.warning(f"âš ï¸ Found {infinite_values} infinite values in avg_value")
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØµÙÙˆÙ Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ NaN ÙÙŠ Ø£Ø¹Ù…Ø¯Ø© Ø£Ø³Ø§Ø³ÙŠØ©
    initial_count = len(df)
    essential_columns = ['date', 'variable', 'avg_value']
    cleaned_df = df.dropna(subset=essential_columns)
    removed_count = initial_count - len(cleaned_df)
    
    if removed_count > 0:
        context.log.warning(f"ğŸ—‘ï¸ Removed {removed_count} weather records with missing essential data")
    
    context.log.info(f"âœ… Weather validation complete. {len(cleaned_df)} valid records remaining")
    
    return cleaned_df


@op
def load_weather_to_snowflake(context, df: pd.DataFrame):
    """
    ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø±Ø·ÙˆØ¨Ø© ÙˆØ¯Ø±Ø¬Ø© Ø§Ù„Ø­Ø±Ø§Ø±Ø© Ø§Ù„ÙŠÙˆÙ…ÙŠØ© Ù„Ù€ Snowflake
    """
    if df.empty:
        context.log.warning("âš ï¸ Empty weather dataframe - skipping load")
        return {"status": "skipped", "loaded_count": 0, "file_info": "empty"}

    context.log.info(f"ğŸ“¤ Preparing to load {len(df)} daily weather records to Snowflake...")
    
    try:
        conn = get_snowflake_connection()
        cur = conn.cursor()
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
        cur.execute("""
            CREATE TABLE IF NOT EXISTS NASA_DAILY_HUMIDITY_TEMPERATURE (
                date DATE,
                year INT,
                month INT,
                day INT,
                variable STRING,
                avg_value FLOAT,
                measurement_count INT,
                loaded_at TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()
            )
        """)
        
        insert_query = """
            INSERT INTO NASA_DAILY_HUMIDITY_TEMPERATURE 
            (date, year, month, day, variable, avg_value, measurement_count) 
            VALUES (%s, %s, %s, %s, %s, %s, %s)
        """
        
        # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© NaN
        successful_inserts = 0
        failed_inserts = 0
        error_messages = []
        
        for index, row in df.iterrows():
            try:
                # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… NaN
                avg_value = row["avg_value"]
                if pd.isna(avg_value) or pd.isnull(avg_value):
                    context.log.warning(f"âš ï¸ Found NaN value in row {index}, skipping this row")
                    failed_inserts += 1
                    continue
                
                measurement_count = row["measurement_count"]
                if pd.isna(measurement_count) or pd.isnull(measurement_count):
                    measurement_count = 0
                
                # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
                if pd.isna(row["date"]) or pd.isna(row["variable"]):
                    context.log.warning(f"âš ï¸ Missing essential data in row {index}, skipping")
                    failed_inserts += 1
                    continue
                
                # Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„ØµÙ
                cur.execute(insert_query, (
                    row["date"], 
                    int(row["year"]), 
                    int(row["month"]), 
                    int(row["day"]), 
                    row["variable"], 
                    float(avg_value),
                    int(measurement_count)
                ))
                successful_inserts += 1
                
            except Exception as row_error:
                failed_inserts += 1
                error_msg = f"Error in row {index}: {str(row_error)}"
                error_messages.append(error_msg)
                context.log.warning(f"âš ï¸ {error_msg}")
                continue  # Ø§Ø³ØªÙ…Ø±Ø§Ø± Ù…Ø¹ Ø§Ù„ØµÙÙˆÙ Ø§Ù„ØªØ§Ù„ÙŠØ©
        
        conn.commit()
        
        # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        if successful_inserts > 0:
            context.log.info(f"âœ… Successfully loaded {successful_inserts} daily weather records")
        
        if failed_inserts > 0:
            context.log.warning(f"âš ï¸ Failed to load {failed_inserts} weather records due to errors")
        
        if error_messages:
            context.log.info("ğŸ“‹ Weather error summary:")
            for msg in error_messages[:5]:  # Ø§Ø·Ø¨Ø¹ Ø£ÙˆÙ„ 5 Ø£Ø®Ø·Ø§Ø¡ ÙÙ‚Ø·
                context.log.info(f"   - {msg}")
            if len(error_messages) > 5:
                context.log.info(f"   ... and {len(error_messages) - 5} more errors")
        
        cur.close()
        conn.close()
        
        return {
            "status": "partial_success" if failed_inserts > 0 else "success",
            "loaded_count": successful_inserts,
            "failed_count": failed_inserts,
            "total_records": len(df)
        }
        
    except Exception as e:
        context.log.error(f"âŒ Critical error loading weather data to Snowflake: {e}")
        return {
            "status": "failed",
            "loaded_count": 0,
            "failed_count": len(df),
            "error": str(e)
        }


@job
def nasa_daily_weather_pipeline():
    """
    Pipeline Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø±Ø·ÙˆØ¨Ø© ÙˆØ¯Ø±Ø¬Ø© Ø§Ù„Ø­Ø±Ø§Ø±Ø© Ø§Ù„ÙŠÙˆÙ…ÙŠØ©
    """
    files = search_nasa_weather_files()
    processed = files.map(process_single_weather_file)
    cleaned = processed.map(clean_weather_dataframe)
    transformed = cleaned.map(transform_weather_data)
    validated = transformed.map(validate_weather_data_before_load)
    validated.map(load_weather_to_snowflake)


if __name__ == "__main__":
    from dagster import execute_pipeline
    result = execute_pipeline(nasa_daily_weather_pipeline)
    print(f"âœ… Weather pipeline finished: {result.success}")
