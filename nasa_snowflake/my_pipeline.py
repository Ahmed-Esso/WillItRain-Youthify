import io
import xarray as xr
import pandas as pd
import snowflake.connector
from dagster import job, op, DynamicOut, DynamicOutput, In
import earthaccess
import os
from typing import Iterator

# ==========================
# Snowflake Config
# ==========================
SNOWFLAKE_ACCOUNT = "KBZQPZO-WX06551"
SNOWFLAKE_USER = "A7MEDESSO"
SNOWFLAKE_AUTHENTICATOR = "externalbrowser"
SNOWFLAKE_ROLE = "ACCOUNTADMIN"
SNOWFLAKE_WAREHOUSE = "NASA_WH"
SNOWFLAKE_DATABASE = "NASA_DB"
SNOWFLAKE_SCHEMA = "PUBLIC"

VARIABLES = ["T2M"]

# Ø­Ø¬Ù… Ø§Ù„Ù€ batch - Ù‚Ù„Ù„Ù‡ Ù„Ùˆ Ø§Ù„Ù…ÙŠÙ…ÙˆØ±ÙŠ Ù‚Ù„ÙŠÙ„
BATCH_SIZE = 50000  # rows per batch

# ==========================
# Helper Function
# ==========================
def get_snowflake_connection():
    """Create Snowflake connection"""
    return snowflake.connector.connect(
        account=SNOWFLAKE_ACCOUNT,
        user=SNOWFLAKE_USER,
        authenticator=SNOWFLAKE_AUTHENTICATOR,
        warehouse=SNOWFLAKE_WAREHOUSE,
        database=SNOWFLAKE_DATABASE,
        schema=SNOWFLAKE_SCHEMA,
        role=SNOWFLAKE_ROLE
    )

# ==========================
# DAGSTER OPS - STREAMING APPROACH
# ==========================

@op(out=DynamicOut())
def search_nasa_files(context):
    """
    Ø¨Ø³ Ø¨Ù†Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ù„ÙØ§Øª - Ù…Ø´ Ø¨Ù†Ù†Ø²Ù„Ù‡Ø§!
    ÙˆÙ†Ø±Ø¬Ø¹ ÙƒÙ„ file ÙƒÙ€ dynamic output Ù…Ù†ÙØµÙ„
    """
    context.log.info("ğŸ” Logging into NASA Earthdata...")
    auth = earthaccess.login(strategy="environment")
    
    context.log.info("ğŸ” Searching for NASA files...")
    results = earthaccess.search_data(
        short_name="M2T1NXSLV",
        version="5.12.4",
        temporal=("2022-01-01", "2023-01-01"),
        bounding_box=(24.70, 22.00, 37.35, 31.67)  # Ø§Ù„Ù‚Ø§Ù‡Ø±Ø©
    )
    
    context.log.info(f"âœ… Found {len(results)} files")
    
    # Ù†Ø±Ø¬Ø¹ ÙƒÙ„ file ÙƒÙ€ dynamic output Ù…Ù†ÙØµÙ„
    for idx, granule in enumerate(results):
        yield DynamicOutput(
            value=granule,
            mapping_key=f"file_{idx}"
        )


@op
def process_single_file(context, granule) -> pd.DataFrame:
    """
    Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ù ÙˆØ§Ø­Ø¯ Ø¨Ø³ - streaming Ù…Ù† Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª Ø¨Ø¯ÙˆÙ† ØªÙ†Ø²ÙŠÙ„
    """
    try:
        context.log.info(f"ğŸ“¥ Streaming file: {granule['meta']['native-id']}")
        
        # Ø¨Ù†ÙØªØ­ Ø§Ù„Ù…Ù„Ù Ù…Ø¨Ø§Ø´Ø±Ø© Ù…Ù† Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª Ø¨Ø¯ÙˆÙ† Ù…Ø§ Ù†Ù†Ø²Ù„Ù‡
        file_stream = earthaccess.open([granule])[0]
        
        # Ù†ÙØªØ­ Ø§Ù„Ù€ dataset
        ds = xr.open_dataset(file_stream, engine="h5netcdf")
        
        all_vars = []
        
        # Ù†Ø¹Ø§Ù„Ø¬ ÙƒÙ„ variable Ù„ÙˆØ­Ø¯Ù‡
        for var in VARIABLES:
            if var in ds.variables:
                # Ù†Ø­ÙˆÙ„ Ù„Ù€ DataFrame
                df = ds[var].to_dataframe().reset_index()
                df["variable"] = var
                
                # Ù†Ø§Ø®Ø¯ Ø£ÙˆÙ„ timestamp Ù…Ù† Ø§Ù„Ù…Ù„Ù
                if "time" in ds.coords:
                    df["timestamp"] = pd.to_datetime(ds.time.values[0])
                
                all_vars.append(df)
        
        ds.close()
        
        if not all_vars:
            context.log.warning(f"âš ï¸ No variables found in file")
            return pd.DataFrame()
        
        # Ù†Ø¬Ù…Ø¹ Ø§Ù„Ù€ variables Ù…Ù† Ø§Ù„Ù…Ù„Ù Ø¯Ù‡ Ø¨Ø³
        combined = pd.concat(all_vars, ignore_index=True)
        context.log.info(f"âœ… Processed {len(combined)} rows from file")
        
        return combined
        
    except Exception as e:
        context.log.error(f"âŒ Error processing file: {e}")
        return pd.DataFrame()


@op
def transform_variables(context, df: pd.DataFrame) -> pd.DataFrame:
    """
    Transformation Ø¨Ø³ÙŠØ· Ø¹Ù„Ù‰ ÙƒÙ„ batch
    """
    if df.empty:
        return df
    
    context.log.info(f"ğŸ”„ Transforming {len(df)} rows...")
    
    # Ù†ØªØ£ÙƒØ¯ Ø¥Ù† timestamp Ù…ÙˆØ¬ÙˆØ¯
    if "timestamp" not in df.columns:
        context.log.warning("âš ï¸ No timestamp column found")
        return pd.DataFrame()
    
    # Ù†Ø­Ø³Ø¨ Ø§Ù„Ù…ØªÙˆØ³Ø· Ù„ÙƒÙ„ variable ÙˆÙ„ÙƒÙ„ Ø´Ù‡Ø±
    transformed = (
        df.groupby(["variable", df["timestamp"].dt.month])
        .mean(numeric_only=True)
        .reset_index()
    )
    
    transformed.rename(columns={"timestamp": "month"}, inplace=True)
    transformed["month"] = transformed["month"].astype(int)
    
    # Ù†Ø§Ø®Ø¯ Ø£ÙˆÙ„ Ø¹Ù…ÙˆØ¯ Ø±Ù‚Ù…ÙŠ Ø¨Ø¹Ø¯ Ø§Ù„Ù€ variable ÙˆØ§Ù„Ù€ month
    numeric_cols = transformed.select_dtypes(include=['float64', 'int64']).columns
    if len(numeric_cols) > 0:
        transformed["avg_value"] = transformed[numeric_cols[0]]
    else:
        context.log.warning("âš ï¸ No numeric columns found")
        return pd.DataFrame()
    
    result = transformed[["variable", "month", "avg_value"]]
    context.log.info(f"âœ… Transformed to {len(result)} rows")
    
    return result


@op
def load_to_snowflake(context, df: pd.DataFrame):
    """
    ØªØ­Ù…ÙŠÙ„ batch ÙˆØ§Ø­Ø¯ Ù„Ù€ Snowflake
    """
    if df.empty:
        context.log.warning("âš ï¸ Empty dataframe - skipping load")
        return "skipped"
    
    context.log.info(f"ğŸ“¤ Loading {len(df)} rows to Snowflake...")
    
    try:
        conn = get_snowflake_connection()
        cur = conn.cursor()
        
        # Ù†Ø¹Ù…Ù„ Ø§Ù„Ù€ table Ù„Ùˆ Ù…Ø´ Ù…ÙˆØ¬ÙˆØ¯Ø© (Ù‡ÙŠØ¹Ù…Ù„ Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© Ø¨Ø³)
        cur.execute("""
            CREATE TABLE IF NOT EXISTS NASA_VARIABLES (
                variable STRING,
                month INT,
                avg_value FLOAT
            )
        """)
        
        # Ù†Ø³ØªØ®Ø¯Ù… batch insert Ø¨Ø¯Ù„ Ù…Ù† row-by-row (Ø£Ø³Ø±Ø¹ Ø¨ÙƒØªÙŠØ±!)
        insert_query = """
            INSERT INTO NASA_VARIABLES (variable, month, avg_value) 
            VALUES (%s, %s, %s)
        """
        
        # Ù†Ø­Ø¶Ø± Ø§Ù„Ø¯Ø§ØªØ§ ÙƒÙ€ list of tuples
        data_to_insert = [
            (row["variable"], int(row["month"]), float(row["avg_value"]))
            for _, row in df.iterrows()
        ]
        
        # batch insert
        cur.executemany(insert_query, data_to_insert)
        conn.commit()
        
        context.log.info(f"âœ… Successfully loaded {len(df)} rows")
        
        cur.close()
        conn.close()
        
        return "success"
        
    except Exception as e:
        context.log.error(f"âŒ Error loading to Snowflake: {e}")
        raise


# ==========================
# DAGSTER JOB - DYNAMIC PIPELINE
# ==========================

@job
def nasa_variables_pipeline():
    """
    Pipeline Ø¨ÙŠØ´ØªØºÙ„ Ø¨Ø·Ø±ÙŠÙ‚Ø© dynamic:
    1. Ø¨Ù†Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ù„ÙØ§Øª
    2. ÙƒÙ„ Ù…Ù„Ù Ø¨ÙŠØªØ¹Ø§Ù„Ø¬ Ù„ÙˆØ­Ø¯Ù‡
    3. Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø¨ØªØªØ­ÙˆÙ„
    4. ÙˆØ¨ØªØªØ±ÙØ¹ Ù„Ù€ Snowflake
    
    ÙƒÙ„ Ù…Ù„Ù Ù…Ø³ØªÙ‚Ù„ - Ù„Ùˆ ÙˆØ§Ø­Ø¯ ÙØ´Ù„ Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª ØªÙƒÙ…Ù„
    """
    files = search_nasa_files()
    
    # ÙƒÙ„ Ù…Ù„Ù Ù‡ÙŠØªØ¹Ø§Ù„Ø¬ ÙÙŠ op Ù…Ù†ÙØµÙ„
    processed = files.map(process_single_file)
    
    # ÙƒÙ„ Ù†ØªÙŠØ¬Ø© Ù‡ØªØªØ­ÙˆÙ„ ÙˆØªØªØ±ÙØ¹
    transformed = processed.map(transform_variables)
    transformed.map(load_to_snowflake)
